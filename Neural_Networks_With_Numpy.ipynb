{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Networks With Numpy",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJVEqETlVamm",
        "colab_type": "code",
        "outputId": "c9569b96-f4f7-411f-a3ca-561de0e1552b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "!wget -O \"dnn_utils.py\" \"https://raw.githubusercontent.com/liqiang311/deeplearning.ai/master/1_Neural%20Networks%20and%20Deep%20Learning/week4/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step/dnn_utils.py\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-18 18:50:25--  https://raw.githubusercontent.com/liqiang311/deeplearning.ai/master/1_Neural%20Networks%20and%20Deep%20Learning/week4/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step/dnn_utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1756 (1.7K) [text/plain]\n",
            "Saving to: ‘dnn_utils.py’\n",
            "\n",
            "\rdnn_utils.py          0%[                    ]       0  --.-KB/s               \rdnn_utils.py        100%[===================>]   1.71K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-02-18 18:50:26 (290 MB/s) - ‘dnn_utils.py’ saved [1756/1756]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nlvoC1r3bsi",
        "colab_type": "text"
      },
      "source": [
        "IMPORT LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nHTyBwS3V4a",
        "colab_type": "code",
        "outputId": "2fd16b50-d9ed-4d85-a5a2-9feb984f2fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from dnn_utils import sigmoid, relu, relu_backward\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOkFQs9U3eVX",
        "colab_type": "code",
        "outputId": "1c7e3900-e5eb-488b-ea37-53a77fa7cc46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JM_5Nk034Xz",
        "colab_type": "code",
        "outputId": "a492074f-4470-4cba-80de-2d74bafbd3d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('Train shape = {}, y = {}'.format(X_train.shape, y_train.shape))\n",
        "print('Test shape = {}, y = {}'.format(X_test.shape, y_train.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape = (60000, 28, 28), y = (60000,)\n",
            "Test shape = (10000, 28, 28), y = (60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOeLVR9c36Ey",
        "colab_type": "code",
        "outputId": "a6dabf3f-c2ef-41c8-8f8c-5d50c5f2a066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "for i in range(9):\n",
        "\t# define subplot\n",
        "\tplt.subplot(330 + 1 + i)\n",
        "\t# plot raw pixel data\n",
        "\tplt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
        " \n",
        "# show the figure\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAD7CAYAAAAFI30bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntjVszbUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivK4gu448RJX",
        "colab_type": "text"
      },
      "source": [
        "#Architecture\n",
        "\n",
        "![Texte alternatif…](https://www.researchgate.net/profile/Facundo_Bre/publication/321259051/figure/fig1/AS:614329250496529@1523478915726/Artificial-neural-network-architecture-ANN-i-h-1-h-2-h-n-o.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1BcegIN6gYD",
        "colab_type": "text"
      },
      "source": [
        "#Forward + Backward\n",
        "\n",
        "![Texte alternatif…](https://github.com/Kulbear/deep-learning-coursera/raw/997fdb2e2db67acd45d29ae418212463a54be06d/Neural%20Networks%20and%20Deep%20Learning/images/final%20outline.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8uZ5W537BcO",
        "colab_type": "text"
      },
      "source": [
        "Parameters Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kaiAvGwQkUp",
        "colab_type": "text"
      },
      "source": [
        "L : layer dims length \\\\\n",
        "layer_dims[l] : Wl first shape, b shape \\\\\n",
        "layer_dims[l-1]: Wl second shape\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQZhge04Qg_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "        print(\"parameter W\" + str(l) + \" shape :\" + str(parameters['W' + str(l)].shape))\n",
        "        print(\"parameter b\" + str(l) + \" shape :\" + str(parameters['b' + str(l)].shape))\n",
        "\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xSK0ZJ8nsnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers_dims = [X_train_flat_std.shape[0], 32, 32, 32, 10] #  5-layer model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J85tptePk6FF",
        "colab_type": "code",
        "outputId": "8415679c-efae-4512-c329-b07dd45d4f78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "parameters = initialize_parameters_deep(layers_dims)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameter W1 shape :(128, 728)\n",
            "parameter b1 shape :(128, 1)\n",
            "parameter W2 shape :(128, 128)\n",
            "parameter b2 shape :(128, 1)\n",
            "parameter W3 shape :(64, 128)\n",
            "parameter b3 shape :(64, 1)\n",
            "parameter W4 shape :(10, 64)\n",
            "parameter b4 shape :(10, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNdAp6Op-sWM",
        "colab_type": "text"
      },
      "source": [
        "Linear Forward "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYy96bX7-5um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter \n",
        "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    #print(\"W shape : \", W.shape, \"A shape : \", A.shape, \"b shape :\", b.shape)\n",
        "    Z = np.dot(W, A) + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNs9-3WSE1fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value \n",
        "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQnaljhXFe-p",
        "colab_type": "text"
      },
      "source": [
        "#Linear + Activation Process\n",
        "\n",
        "![Texte alternatif…](https://github.com/Kulbear/deep-learning-coursera/raw/997fdb2e2db67acd45d29ae418212463a54be06d/Neural%20Networks%20and%20Deep%20Learning/images/model_architecture_kiank.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7nrOgd1FQsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: L_model_forward\n",
        "\n",
        "def L_model_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    Returns:\n",
        "    AL -- last post-activation value\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
        "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, \n",
        "                                            parameters['W' + str(l)], \n",
        "                                            parameters['b' + str(l)], \n",
        "                                            activation='relu')\n",
        "        caches.append(cache)\n",
        "    \n",
        "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "    AL, cache = linear_activation_forward(A, \n",
        "                                         parameters['W' + str(L)], \n",
        "                                         parameters['b' + str(L)], \n",
        "                                         activation='sigmoid')\n",
        "    caches.append(cache)\n",
        "    assert(AL.shape == (10, X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCKIgkfVKpZ9",
        "colab_type": "text"
      },
      "source": [
        "Loss Function $J$ Formula :\n",
        "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)$$\n",
        "![Texte alternatif…](https://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V528FeKgJIhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function.\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMMKDcAiJk5J",
        "colab_type": "text"
      },
      "source": [
        "#BACKWARD PROPAGATION MODULE\n",
        "\n",
        "![Texte alternatif…](https://github.com/Kulbear/deep-learning-coursera/raw/997fdb2e2db67acd45d29ae418212463a54be06d/Neural%20Networks%20and%20Deep%20Learning/images/backprop_kiank.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDsKQl49KamY",
        "colab_type": "text"
      },
      "source": [
        "#LINEAR BACKWARD\n",
        "\n",
        "![Texte alternatif…](https://github.com/Kulbear/deep-learning-coursera/raw/997fdb2e2db67acd45d29ae418212463a54be06d/Neural%20Networks%20and%20Deep%20Learning/images/linearback_kiank.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBlePfOjLXmk",
        "colab_type": "text"
      },
      "source": [
        "![Texte alternatif…](https://render.githubusercontent.com/render/math?math=dW%5E%7B%5Bl%5D%7D%20%3D%20%5Cfrac%7B%5Cpartial%20%5Cmathcal%7BL%7D%20%7D%7B%5Cpartial%20W%5E%7B%5Bl%5D%7D%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%20dZ%5E%7B%5Bl%5D%7D%20A%5E%7B%5Bl-1%5D%20T%7D%20%5Ctag%7B8%7D&mode=display)\n",
        "\n",
        "\n",
        "![Texte alternatif…](https://render.githubusercontent.com/render/math?math=db%5E%7B%5Bl%5D%7D%20%3D%20%5Cfrac%7B%5Cpartial%20%5Cmathcal%7BL%7D%20%7D%7B%5Cpartial%20b%5E%7B%5Bl%5D%7D%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%20%5Csum_%7Bi%20%3D%201%7D%5E%7Bm%7D%20dZ%5E%7B%5Bl%5D%28i%29%7D%5Ctag%7B9%7D&mode=display)\n",
        "\n",
        "![Texte alternatif…](https://render.githubusercontent.com/render/math?math=dA%5E%7B%5Bl-1%5D%7D%20%3D%20%5Cfrac%7B%5Cpartial%20%5Cmathcal%7BL%7D%20%7D%7B%5Cpartial%20A%5E%7B%5Bl-1%5D%7D%7D%20%3D%20W%5E%7B%5Bl%5D%20T%7D%20dZ%5E%7B%5Bl%5D%7D%20%5Ctag%7B10%7D&mode=display)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_7s_sDBJeSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = np.dot(dZ, A_prev.T) / m\n",
        "    db = np.squeeze(np.sum(dZ, axis=1, keepdims=True)) / m\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    #assert (isinstance(db, float))\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwpQ2T2YMB7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l \n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        #print(\"linear activation backward activation cache\", activation_cache.shape)\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "    \n",
        "    # Shorten the code\n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRvOAFugMsV5",
        "colab_type": "text"
      },
      "source": [
        "#Linear Backward\n",
        "\n",
        "![Texte alternatif…](https://github.com/Kulbear/deep-learning-coursera/raw/997fdb2e2db67acd45d29ae418212463a54be06d/Neural%20Networks%20and%20Deep%20Learning/images/mn_backward.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYQRs7ImNQvY",
        "colab_type": "text"
      },
      "source": [
        "All L Model Backward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbqylM2TMl0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    current_cache = caches[-1]\n",
        "    #print('dAL shape',dAL.shape)\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAey07y_NXZX",
        "colab_type": "text"
      },
      "source": [
        "#Update Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybd0DY_GNUWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
        "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)].reshape(grads[\"db\" + str(l + 1)].shape[0], 1)\n",
        "        #print(\"gives:\",parameters[\"b\" + str(l+1)].shape)\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5F9FtIAOhFz",
        "colab_type": "text"
      },
      "source": [
        "RESHAPE X_train, X_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MxE7ykLXeY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_l = to_categorical(y_train).T\n",
        "y_test_l = to_categorical(y_test).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ40fKAaOVVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape the training and test examples \n",
        "X_train_flat = X_train.reshape(X_train.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "X_test_flat = X_test.reshape(X_test.shape[0], -1).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1hUD3Y3OzVU",
        "colab_type": "code",
        "outputId": "60becabe-15e5-41e5-d804-235cb6f59495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('Train shape = {}, y = {}'.format(X_train_flat.shape, y_train_l.shape))\n",
        "print('Test shape = {}, y = {}'.format(X_test_flat.shape, y_test_l.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape = (784, 60000), y = (10, 60000)\n",
            "Test shape = (784, 10000), y = (10, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhCnwryKO8xB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardize data to have feature values between 0 and 1.\n",
        "X_train_flat_std = X_train_flat / 255.\n",
        "X_test_flat_std = X_test_flat / 255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLvDGvbsRdpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization.\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "\n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "        #print(i)\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "\n",
        "        # Compute cost.\n",
        "        cost = compute_cost(AL, Y)\n",
        "\n",
        "        # Backward propagation.\n",
        "        #print('Y shape: ', Y.shape)\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "\n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeApAIZER59U",
        "colab_type": "text"
      },
      "source": [
        "Train & Plot Loss :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnmGUxSPZ-PG",
        "colab_type": "code",
        "outputId": "41262c63-63af-47fc-d26d-45134e04caad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X_train_flat_std.shape, y_train_l.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 60000) (10, 60000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQPtmu_ql0Sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single SIGMOID unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    #print(\"DA shap\", dA.shape)\n",
        "    #print(\"Z\", Z.shape)\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyNLOI4ASIwb",
        "colab_type": "code",
        "outputId": "3dc1984e-05f2-4ba8-f986-8efa17d7997f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "source": [
        "parameters = L_layer_model(X_train_flat_std, y_train_l, layers_dims, num_iterations=2500, print_cost=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameter W1 shape :(128, 784)\n",
            "parameter b1 shape :(128, 1)\n",
            "parameter W2 shape :(128, 128)\n",
            "parameter b2 shape :(128, 1)\n",
            "parameter W3 shape :(64, 128)\n",
            "parameter b3 shape :(64, 1)\n",
            "parameter W4 shape :(10, 64)\n",
            "parameter b4 shape :(10, 1)\n",
            "Cost after iteration 0: 6.931509\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-00979ea2f8f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_flat_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-dda23986325d>\u001b[0m in \u001b[0;36mL_layer_model\u001b[0;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Backward propagation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#print('Y shape: ', Y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Update parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-c186d4aaf6df>\u001b[0m in \u001b[0;36mL_model_backward\u001b[0;34m(AL, Y, caches)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mcurrent_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mdA_prev_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dA\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dA\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdA_prev_temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdW_temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-1e6e76a856b2>\u001b[0m in \u001b[0;36mlinear_activation_backward\u001b[0;34m(dA, cache, activation)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Shorten the code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-bee7a0cb1e69>\u001b[0m in \u001b[0;36mlinear_backward\u001b[0;34m(dZ, cache)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2sBPLGJvO-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X, y, parameters):\n",
        "  m = X.shape[1]\n",
        "  n = len(parameters)\n",
        "  p = np.zeros((10, m))\n",
        "  missed = 0\n",
        "  probas, caches = L_model_forward(X, parameters)\n",
        "\n",
        "  for i in range(probas.shape[1]):\n",
        "    indexMax = probas[:, i].argmax()\n",
        "    p[indexMax, i] = 1\n",
        "    if(indexMax != y.argmax()):\n",
        "      missed += 1\n",
        "  print(\"accuracy = \", 1 - missed/m)\n",
        "  return p, missed, m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH5n6ITGxJqC",
        "colab_type": "code",
        "outputId": "70299a53-966a-44fe-803c-19bab9de939e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions, missed, m = predict(X_test_flat_std, y_test_l, parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy =  0.0039000000000000146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSCujoPexPW-",
        "colab_type": "code",
        "outputId": "17723f59-c60e-4c36-c4c9-e98f140d1bdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "missed"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9961"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik95N5KpzYQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}